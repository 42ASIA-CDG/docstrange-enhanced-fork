"""Qwen2-VL processor using vLLM for faster inference (PRODUCTION-READY).

This processor uses vLLM instead of transformers for:
- 2-3x faster inference
- Better GPU memory management
- Higher throughput for batch processing
- Optimized KV cache

Installation:
    pip install vllm>=0.6.0

Note: vLLM requires CUDA 11.8+ and is significantly faster than transformers.
"""

import logging
import json
import re
from typing import Dict, Any, Optional, List
import os
from PIL import Image
import base64
from io import BytesIO

logger = logging.getLogger(__name__)


class Qwen2VLvLLMProcessor:
    """Qwen2-VL model processor using vLLM for production-grade performance."""
    
    def __init__(self, model_path: str = "Qwen/Qwen2-VL-7B-Instruct"):
        """Initialize Qwen2-VL processor with vLLM.
        
        Args:
            model_path: HuggingFace model path
        """
        self.model_path = model_path
        self.llm = None
        self._initialize_model()
    
    def _initialize_model(self):
        """Load Qwen2-VL model with vLLM."""
        try:
            from vllm import LLM, SamplingParams
            
            logger.info(f"Loading Qwen2-VL with vLLM from {self.model_path}")
            print(f"ðŸš€ Loading Qwen2-VL with vLLM (7B) - optimized for speed...")
            
            # vLLM configuration for vision-language model
            self.llm = LLM(
                model=self.model_path,
                dtype="half",  # Use FP16 for speed
                gpu_memory_utilization=0.85,  # Use up to 85% of GPU memory
                max_model_len=4096,  # Context length
                trust_remote_code=True,  # Required for Qwen models
            )
            
            # Define default sampling parameters
            self.sampling_params = SamplingParams(
                temperature=0.0,  # Deterministic for structured extraction
                top_p=1.0,
                max_tokens=8192,  # Increased to handle large OCR outputs
                stop=["<|im_end|>", "<|endoftext|>"],
            )
            
            logger.info("Qwen2-VL vLLM model loaded successfully")
            print(f"âœ… Qwen2-VL vLLM ready (2-3x faster than transformers)")
            
        except ImportError as e:
            logger.error(f"vLLM not installed: {e}")
            raise ImportError(
                "vLLM is required for this processor. Install with: pip install vllm>=0.6.0"
            )
        except Exception as e:
            logger.error(f"Failed to initialize vLLM model: {e}")
            raise
    
    def _image_to_base64(self, image_path: str) -> str:
        """Convert image to base64 string for vLLM.
        
        Args:
            image_path: Path to image file
            
        Returns:
            Base64 encoded image string
        """
        image = Image.open(image_path).convert("RGB")
        buffered = BytesIO()
        image.save(buffered, format="PNG")
        return base64.b64encode(buffered.getvalue()).decode()
    
    def extract_text(self, image_path: str) -> str:
        """Extract text from image using vLLM.
        
        Args:
            image_path: Path to image file
            
        Returns:
            Extracted text in markdown format
        """
        try:
            image_b64 = self._image_to_base64(image_path)
            
            prompt = """<|im_start|>system
You are a helpful assistant that extracts text from documents.<|im_end|>
<|im_start|>user
<image>
Extract all text from this document in markdown format. Preserve the layout and structure.<|im_end|>
<|im_start|>assistant
"""
            
            # vLLM inference
            outputs = self.llm.generate(
                prompt,
                sampling_params=self.sampling_params,
                use_tqdm=False
            )
            
            return outputs[0].outputs[0].text.strip()
            
        except Exception as e:
            logger.error(f"Text extraction failed: {e}")
            raise
    
    def extract_structured_data(self, image_path: str, json_schema: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """Extract structured data using vLLM (FAST).
        
        Args:
            image_path: Path to image file
            json_schema: Optional JSON schema to guide extraction
            
        Returns:
            Structured data as dictionary
        """
        try:
            image_b64 = self._image_to_base64(image_path)
            
            # Build prompt based on schema
            if json_schema:
                schema_str = json.dumps(json_schema, indent=2)
                user_message = f"""<image>

You are a document extraction AI. Extract information from this document image and return ONLY a valid JSON object.

Schema to follow:
{schema_str}

IMPORTANT RULES:
1. Return ONLY the JSON object - no markdown, no code blocks, no explanations
2. Start your response with {{ and end with }}
3. Extract the value for every field in the schema
4. For "tags", return an array of strings like ["tag1", "tag2"]
5. For "summary", write a clear 2-3 sentence description
6. For "file_type", identify the document type from the schema description
7. For "full_doc_ocr": Extract ALL visible text preserving layout. Convert Arabic-Indic numerals (Ù Ù¡Ù¢Ù£Ù¤Ù¥Ù¦Ù§Ù¨Ù©) to Western numerals (0123456789). Keep Arabic/non-Latin text as-is.
8. AVOID repetition - if you see repeated values, write them only once
9. Do not add any text before or after the JSON

JSON output:"""
            else:
                user_message = """<image>

Extract all information from this document and return it as a structured JSON object.
Return ONLY the JSON, no explanation."""
            
            # Construct vLLM-compatible prompt
            prompt = f"""<|im_start|>system
You are a helpful assistant that extracts structured data from documents.<|im_end|>
<|im_start|>user
{user_message}<|im_end|>
<|im_start|>assistant
"""
            
            logger.info("ðŸ” Starting vLLM structured extraction...")
            print("âš¡ Generating structured data with vLLM (FAST - 5-10 seconds)...")
            
            # vLLM inference
            outputs = self.llm.generate(
                prompt,
                sampling_params=self.sampling_params,
                use_tqdm=False
            )
            
            json_text = outputs[0].outputs[0].text.strip()
            
            print(f"âœ… vLLM generation complete. Response length: {len(json_text)} chars")
            
            # Parse JSON
            parsed = self._parse_json(json_text)
            
            return {
                "structured_data": parsed,
                "format": "qwen2vl_vllm_structured_json",
                "model": "qwen2vl_vllm",
                "schema": json_schema
            }
            
        except Exception as e:
            logger.error(f"Structured extraction failed: {e}")
            import traceback
            traceback.print_exc()
            raise
    
    def _parse_json(self, text: str) -> Dict[str, Any]:
        """Parse JSON from text, handling markdown code blocks.
        
        Args:
            text: Text containing JSON
            
        Returns:
            Parsed JSON dictionary
        """
        logger.info(f"[vLLM_PARSE] Attempting to parse JSON from output (length: {len(text)})")
        logger.debug(f"[vLLM_PARSE] First 500 chars: {text[:500]}")
        
        # Try to extract JSON from markdown code blocks
        json_match = re.search(r'```(?:json)?\s*\n(.*?)\n```', text, re.DOTALL)
        if json_match:
            text = json_match.group(1).strip()
            logger.info("[vLLM_PARSE] Extracted JSON from markdown code block")
        
        # Try direct parsing
        try:
            parsed = json.loads(text)
            logger.info(f"[vLLM_PARSE] Successfully parsed JSON with {len(parsed)} keys")
            return parsed
        except json.JSONDecodeError as e:
            logger.warning(f"[vLLM_PARSE] Direct parsing failed: {e}")
            # Try cleaning
            try:
                cleaned = text.strip('`').strip()
                parsed = json.loads(cleaned)
                logger.info("[vLLM_PARSE] Successfully parsed after cleaning")
                return parsed
            except json.JSONDecodeError:
                # Last resort: try to find JSON object
                json_match = re.search(r'\{.*\}', text, re.DOTALL)
                if json_match:
                    try:
                        parsed = json.loads(json_match.group(0))
                        logger.info("[vLLM_PARSE] Successfully extracted JSON object from text")
                        return parsed
                    except Exception as inner_e:
                        logger.error(f"[vLLM_PARSE] Failed to parse extracted JSON: {inner_e}")
                
                logger.error(f"[vLLM_PARSE] All parsing attempts failed. Raw output: {text[:500]}")
                return {"error": "Failed to parse JSON", "raw_output": text[:1000]}
    
    def is_available(self) -> bool:
        """Check if vLLM model is available.
        
        Returns:
            True if model is loaded
        """
        return self.llm is not None
