# GPU Memory Requirements

## ‚ö†Ô∏è Important: Your GPU has 7.6GB VRAM

Based on your GPU memory (7.6GB), here's what works:

## ‚úÖ Models That Work on Your GPU

### 1. Donut (RECOMMENDED) üç©
- **Memory**: ~2-3GB
- **Speed**: 2-5s per page
- **Best For**: Invoices, receipts, forms
- **Status**: ‚úÖ Works perfectly on your GPU

```python
extractor = DocumentExtractor(model="donut")
```

### 2. Nanonets
- **Memory**: ~6-7GB (tight fit)
- **Speed**: 10-30s per page
- **Best For**: General documents
- **Status**: ‚úÖ Should work (close to limit)

```python
extractor = DocumentExtractor(model="nanonets")
```

## ‚ùå Models That DON'T Work on Your GPU

### 3. Qwen2-VL
- **Memory Required**: ~7-8GB
- **Your GPU**: 7.6GB (not enough)
- **Status**: ‚ùå Out of memory errors
- **Solution**: Need 10GB+ VRAM or use CPU (very slow)

### 4. Phi-3-Vision
- **Memory Required**: ~5-6GB
- **Your GPU**: 7.6GB
- **Status**: ‚ö†Ô∏è Might work with optimizations, but tight

## üí° Recommendations for Your System

### Best Choice: Use Donut
```bash
python test_gpu_with_image.py
# Select option 2 (Donut)
```

**Why Donut?**
- ‚úÖ 5-10x faster than Qwen2-VL
- ‚úÖ Uses only 2-3GB (plenty of room)
- ‚úÖ No memory errors
- ‚úÖ Great for invoices/receipts

### Alternative: Use Nanonets
```bash
python test_gpu_with_image.py
# Select option 1 (Nanonets)
```

**Nanonets pros/cons:**
- ‚úÖ Higher accuracy
- ‚úÖ General purpose
- ‚ö†Ô∏è Uses ~6-7GB (close to your limit)
- ‚è≥ Slower (10-30s)

## üîß What I've Done to Help

I've optimized Qwen2-VL and Phi-3-Vision to use less memory:
- Changed from bfloat16 to float16 (saves ~15%)
- Added memory limit: `max_memory={0: "6GB"}`
- Reduced max tokens: 1024 instead of 2048
- Added `num_beams=1` (faster, less memory)

**But even with optimizations, Qwen2-VL needs more than 7.6GB.**

## üéØ What You Should Do

### For Quick Testing (Recommended)
```bash
# Use Donut - it's fast and works great on your GPU
python test_gpu_with_image.py
# Select: 2 (Donut)
```

### For Production Use

**Option 1: Use Donut (Fast & Efficient)**
```python
from docstrange import DocumentExtractor

extractor = DocumentExtractor(model="donut")
result = extractor.extract("invoice.pdf")
data = result.extract_data(json_schema=schema)
```

**Option 2: Use Nanonets (High Accuracy)**
```python
extractor = DocumentExtractor(model="nanonets")
result = extractor.extract("document.pdf")
```

## üìä Memory Usage Breakdown

| Model | Parameters | FP16 Size | Runtime Memory | Fits 7.6GB? |
|-------|-----------|-----------|----------------|-------------|
| **Donut** | 200M | ~400MB | 2-3GB | ‚úÖ YES |
| **Nanonets** | 7B | ~13GB | 6-7GB | ‚úÖ YES (tight) |
| **Phi-3-Vision** | 4.2B | ~8GB | 5-6GB | ‚ö†Ô∏è MAYBE |
| **Qwen2-VL** | 7B | ~14GB | 7-8GB | ‚ùå NO |

**Runtime memory** = Model weights + activations + gradients + KV cache

## üöÄ Upgrade Options (If You Need Qwen2-VL)

If you really need Qwen2-VL's advanced features:

### Option 1: Cloud GPU
- Google Colab (free): 15GB VRAM
- Lambda Labs: RTX 3090/4090 (24GB)
- RunPod: Rent by the hour

### Option 2: Upgrade GPU
- RTX 3060 (12GB) - Budget option
- RTX 4070 (12GB) - Good balance  
- RTX 4090 (24GB) - Run all models

### Option 3: CPU Inference (Not Recommended)
```python
# Very slow but works
import os
os.environ["CUDA_VISIBLE_DEVICES"] = ""  # Disable GPU

extractor = DocumentExtractor(model="qwen2vl")
# This will be 10-20x slower
```

## üéÅ The Good News

**Donut is PERFECT for your GPU!**
- Fast (2-5s vs 10-30s)
- Efficient (2-3GB vs 7-8GB)
- Accurate for invoices/receipts
- No memory issues

**For 90% of use cases, Donut is the best choice anyway!** üéâ

## üß™ Test Script

Here's a quick test to see what fits:

```python
import torch

print(f"GPU Memory Available: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")

# Test Donut (should work)
try:
    extractor = DocumentExtractor(model="donut")
    print("‚úÖ Donut loaded successfully")
except Exception as e:
    print(f"‚ùå Donut failed: {e}")

# Test Nanonets (should work but tight)
try:
    torch.cuda.empty_cache()
    extractor = DocumentExtractor(model="nanonets")
    print("‚úÖ Nanonets loaded successfully")
except Exception as e:
    print(f"‚ùå Nanonets failed: {e}")
```

## üìù Summary

**Your GPU (7.6GB) is perfect for:**
- ‚úÖ Donut (fast & efficient)
- ‚úÖ Nanonets (accurate, but slower)

**Your GPU is too small for:**
- ‚ùå Qwen2-VL (needs 10GB+)
- ‚ö†Ô∏è Phi-3-Vision (might work with optimizations)

**Recommendation: Use Donut for 90% of tasks, Nanonets for the other 10%.**
